{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NOTEBOOKE CONTENT\n\nIn the classification task, we focus on the content of the sentence to demonstrate what we will do.\n\nThis notebook is divided into two parts. First, we will use the Naive Bayes model to classify the descriptions of films. In the second part, we will focus on prompt engineering. Using the large language model `FLAN-T5,` we will present the results and evaluate the model's performance. Finally, we will employ fine-tuning methods to achieve good results.\n\n\n\n### \"Note: In a practical context, one should use either of them, not both. When you isolate one, you will not encounter an error.\" so you can do Part-1 or Part-2\n\n## Part-1:\n- [1.0 - Setup And Import The Requirements.](#1)\n- [2.0 - Load Data And Process it.](#2)\n    - [2.1- Load Data ](#2.1)\n    - [2.2- Cleaning The Data](#2.2)\n    - [2.3- Feature Engineering](#2.3)\n    - [2.4- Do TFIDF Method](#2.4)\n- [3.0 - Load Model And Make Prediction](#3.0)\n    - [3.1- Load Models And Choose The Best](#3.1)\n    - [3.2- Make prediction](#3.2)\n\n## Part-2:\n\n- [ 1 -Load Required Dependencies, Dataset and LLM](#1)\n  - [ 1.1 - Test the Model with Zero Shot Inferencing](#1.1)\n  - [ 1.2 - Using One Shot and Few Shot Inference ](#1.2)\n- [ 2 - Perform Parameter Efficient Fine-Tuning (PEFT)](#2)\n  - [2.1 - Preprocess the Classification Dataset](#2.1)\n  - [ 2.2 - Setup the PEFT/LoRA model for Fine-Tuning](#2.2)\n  - [ 2.3 - Train PEFT Adapter](#2.3)\n  - [ 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n  \n- [ 3 - submission ](#1)","metadata":{}},{"cell_type":"markdown","source":"# 1.0 - Setup And Import The Requirements.\nlet's setup some important libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport string\nimport warnings\nimport nltk\nimport spacy\nimport sklearn\nimport unicodedata\nimport os\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize , sent_tokenize\nfrom nltk.stem import LancasterStemmer\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom wordcloud import WordCloud , STOPWORDS\nfrom bs4 import BeautifulSoup\nfrom textblob import TextBlob , Word\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report , confusion_matrix , accuracy_score\n\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:35:06.049996Z","iopub.execute_input":"2023-10-04T01:35:06.050391Z","iopub.status.idle":"2023-10-04T01:35:09.349299Z","shell.execute_reply.started":"2023-10-04T01:35:06.050345Z","shell.execute_reply":"2023-10-04T01:35:09.347960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.0 - Load Data And Process it.","metadata":{}},{"cell_type":"markdown","source":"# 2.1- Load Data\n\n\nfrom discription file we got\n\n` train data formate ` :\n\nTrain data:\nID ::: TITLE ::: GENRE ::: DESCRIPTION\n\n` test data formate ` :\n\nTest data:\nID ::: TITLE ::: DESCRIPTION","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n\ntrain_file= \"/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/train_data.txt\"\ntest_file = \"/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data.txt\"\ntest_solution= \"/kaggle/input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data_solution.txt\"\n\n\ntry:\n    # Read the CSV file into a DataFrame\n\n    train_data = pd.read_csv(train_file ,  sep=\":::\" , engine=\"python\" , names=['ID' , 'TITLE' , 'GENRE' , \"DESCRIPTION\"] )\n    print(\"Sample data from data.csv:\")\n    train_data.head()\nexcept FileNotFoundError:\n    print(f\"'data.csv' not found . Please adjust the file name or directory path as needed.\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:35:09.351120Z","iopub.execute_input":"2023-10-04T01:35:09.352223Z","iopub.status.idle":"2023-10-04T01:35:10.293182Z","shell.execute_reply.started":"2023-10-04T01:35:09.352171Z","shell.execute_reply":"2023-10-04T01:35:10.291611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['DESCRIPTION'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.GENRE.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.2- Cleaning Data","metadata":{}},{"cell_type":"code","source":"# preprocessing\n\n#let's do some steps\n\n#1. remove HTML\n#2. remove squer prackets\n#3. remove special characters\n#4. remove stopwords\n\n\n# finally collect all functions in one preprocessing function\n\n\ndef remove_html(text):\n    soup = BeautifulSoup(text , 'html.parser')\n    return soup.get_text()\n\ndef remove_squer_prackets(text):\n    return re.sub('\\[[^]]*\\]','',text)\n\ndef remove_special_char(text):\n    return re.sub('[^a-zA-Z0-9\\s]','' , text)\n\n\ndef stemming(text):\n    stem = nltk.porter.PorterStemmer()\n    text = ' '.join([stem.stem(word) for word in text.split()])\n    return text\n\ndef remove_stopwords(text):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    filtering = [word for word in tokens if word.lower() not in stopwords]\n    return ' '.join(filtering)\n\n# collecte\ndef preprocessing(text):\n    docs = remove_html(text)\n    docs = remove_squer_prackets(docs)\n    docs = remove_special_char(docs)\n    docs = stemming(docs)\n    docs = remove_stopwords(docs)\n    return docs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.3- Feature Engineering\n\nAdding year column to show relation between year and kind of filme.\n\n\nAt the end of the film's name, we will find the year of the film, so let's take it and put it in another column.","metadata":{}},{"cell_type":"code","source":"def year_find(text):\n    year_pattern = r'\\((\\d{4})\\)'\n    year = re.search(year_pattern , str(text))\n    if year:\n        extracted_year = year.group(1)\n        return int(extracted_year)\n    else:\n        extracted_year = None\n        return extracted_year\n\n\nprint(int(year_find('Oscar et la dame rose (2009)')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\n\n\ntokenizer = ToktokTokenizer()\nnlp = spacy.load('en_core_web_sm')\nstopwords = list(nlp.Defaults.stop_words)\n\n\ntrain_copy = train_data.copy()\ntrain_copy['DESCRIPTION'] = train_copy['DESCRIPTION'].apply(preprocessing)\ntrain_copy.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract year\n\ntrain_copy['YEAR'] = train_copy['TITLE'].apply(year_find)\ntrain_copy = train_copy.dropna()\ntrain_copy['YEAR'] = train_copy['YEAR'].astype(int)\ntrain_copy.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_copy.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_films=list(train_copy['GENRE'].unique())\nlen(list_films)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_films","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will add year to discription befor doing TFIDF\n\n\ntrain_copy['YEAR'] = train_copy['YEAR'].astype(str)\n\ntrain_copy['discription'] =  train_copy['YEAR'] + ' '+ train_copy['DESCRIPTION']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_copy['discription'][88]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split train data into train and validation sets\n\n\n# convert Genre column to classes values to classiffy it\n\nlabel_encode =  LabelEncoder()\nlabels = label_encode.fit_transform(train_copy['GENRE'])\n\ntrain_set , val_set , train_label , val_label = train_test_split(train_copy['discription'] , labels , test_size=0.2 , shuffle=True , random_state = 42)\n\nline_dash = '-'.join('' for _ in range(100))\n\nprint(line_dash)\nprint(f'Size of train data: {train_copy.shape[0]}')\nprint(line_dash)\nprint(f'Split data into train and eval sets')\nprint(f'Trani Set\\t: {len(train_set)}\\nValidation Set\\t: {len(val_set)}')\nprint(line_dash)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.4- Do TFIDF Method\n","metadata":{}},{"cell_type":"code","source":"tfidf_model = TfidfVectorizer(ngram_range=(1,3) , use_idf=True , min_df = 0 , max_df=1)\n\ntf_train = tfidf_model.fit_transform(train_set)\ntf_val   = tfidf_model.transform(val_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.0 - Load Model And Make Prediction\n","metadata":{}},{"cell_type":"markdown","source":"# 3.1- Load Models And Choose The Best\n\nin this classification task, I will use three models and compare between them by evaluate each of them\n\nusing GridSearch method to get optimial hyperparameters:\n\n`models` :\n\n- Naive Bayes\n- Logistic Regression\n- Support Vector Machine `SVM`\n\n`Let's initialize hyperparameters for each of them.`\n","metadata":{}},{"cell_type":"code","source":"# Create a MultinomialNB model\nNB_model = MultinomialNB()\n\n# Define the hyperparameters and their possible values\nparam_grid = {\n    'alpha': [0.1, 1.0, 10.0],\n    'fit_prior': [True, False]\n}\n\n# Create a grid search object with cross-validation\ngrid_search = GridSearchCV(estimator=NB_model, param_grid=param_grid, cv=5, scoring='accuracy')\n\n# Fit the grid search to your data\ngrid_search.fit(tf_train, train_label)\n\n# Get the best hyperparameters\nbest_alpha = grid_search.best_params_['alpha']\nbest_fit_prior = grid_search.best_params_['fit_prior']\n\n# Use the best hyperparameters to create your final model\nfinal_NB_model = MultinomialNB(alpha=best_alpha, fit_prior=best_fit_prior)\n\n# Train your final model on the entire training dataset\nfinal_NB_model.fit(tf_train , train_label)\n\n\nNB_prediction = final_NB_model.predict(tf_val)\nNB_accuracy   = accuracy_score(NB_prediction , val_label)\n\nprint(f'Naive Bayes accuracy : {NB_accuracy}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"note: LogisticRegression and SVC take time more than Naive Bayes, and give the same result so let's comments them.\n\nbut if you have a time, do it.","metadata":{}},{"cell_type":"code","source":"# LR_model = LogisticRegression()\n\n# # Define the hyperparameters and their possible values\n# param_grid = {\n#     'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n#     'penalty': ['l1', 'l2'],  # Regularization type (L1 or L2)\n#     'solver': ['liblinear']  # Solver for L1 regularization\n# }\n\n# # Create a grid search object with cross-validation\n# LR_grid_search = GridSearchCV(estimator =LR_model ,param_grid=param_grid, cv=5, scoring='accuracy'  )\n\n# LR_grid_search.fit(tf_train , train_label)\n\n# # Get the best hyperparameters\n# best_C = grid_search.best_params_['C']\n# best_penalty = grid_search.best_params_['penalty']\n\n# final_LR_model = LogisticRegression(C=best_C, penalty=best_penalty, solver='liblinear')\n\n# # Train your final model on the entire training dataset\n# final_LR_model.fit(tf_train , train_label)\n# # Get The Accuracy\n# LR_prediction = final_LR_model.predict(tf_val)\n# LR_accuracy   = accuracy_score(LR_prediction , val_label)\n\n# print(f'Logistic Regression accuracy : {LR_accuracy}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create an SVM model\n# svm_model = SVC()\n\n# # Define the hyperparameters and their possible values\n# param_grid = {\n#     'C': [0.1, 1, 10],  # Regularization parameter\n#     'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Kernel type\n#     'degree': [2, 3],  # Degree of the polynomial kernel (if using poly)\n#     'gamma': ['scale', 'auto'] + [0.001, 0.01, 0.1, 1],  # Kernel coefficient (if using poly, rbf, or sigmoid)\n# }\n\n# # Create a grid search object with cross-validation\n# grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy')\n\n# # Fit the grid search to your data\n# grid_search.fit(tf_train, train_label)  # Replace X_train and y_train with your training data\n\n# # Get the best hyperparameters\n# best_C = grid_search.best_params_['C']\n# best_kernel = grid_search.best_params_['kernel']\n# best_degree = grid_search.best_params_['degree']\n# best_gamma = grid_search.best_params_['gamma']\n\n# # Use the best hyperparameters to create your final model\n# final_svm_model = SVC(C=best_C, kernel=best_kernel, degree=best_degree, gamma=best_gamma)\n\n# # Train your final model on the entire training dataset\n# final_svm_model.fit(tf_train, train_label)\n\n# # Get The Accuracy\n# svm_prediction = final_svm_model.predict(tf_val)\n# svm_accuracy   = accuracy_score(svm_prediction , val_label)\n\n# print(f'Support Vector Machine accuracy : {svm_accuracy}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [Part-2](#2)","metadata":{}},{"cell_type":"code","source":"%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.11.0 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    loralib==0.1.1 \\\n    peft==0.3.0 --quiet","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:31:04.938699Z","iopub.execute_input":"2023-10-04T01:31:04.939909Z","iopub.status.idle":"2023-10-04T01:34:38.396067Z","shell.execute_reply.started":"2023-10-04T01:31:04.939826Z","shell.execute_reply":"2023-10-04T01:34:38.393903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport warnings\nimport time\n# import evaluate\n\nfrom pathlib import Path\nfrom string import Template\nfrom transformers import AutoModelForSeq2SeqLM , AutoTokenizer , GenerationConfig , Trainer , TrainingArguments , T5Tokenizer, T5ForConditionalGeneration\nwarnings.simplefilter('ignore')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:34:38.400391Z","iopub.execute_input":"2023-10-04T01:34:38.400818Z","iopub.status.idle":"2023-10-04T01:34:53.299232Z","shell.execute_reply.started":"2023-10-04T01:34:38.400789Z","shell.execute_reply":"2023-10-04T01:34:53.297562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## load Model and Tokenizer\n","metadata":{}},{"cell_type":"code","source":"model_name = 'google/flan-t5-base'\n\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name , torch_dtype = torch.bfloat16)\ntokenizer      = AutoTokenizer.from_pretrained(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:34:53.300690Z","iopub.execute_input":"2023-10-04T01:34:53.301534Z","iopub.status.idle":"2023-10-04T01:35:06.032981Z","shell.execute_reply.started":"2023-10-04T01:34:53.301496Z","shell.execute_reply":"2023-10-04T01:35:06.031403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### let's show number of trainable parameters\n\n\nIt is possible to pull out the number of model parameters and find out how many of them are trainable","metadata":{}},{"cell_type":"code","source":"def print_number_of_trainable_parameters(model):\n    trainable = 0\n    all_params= 0\n    for _,param in model.named_parameters():\n        all_params+= param.numel()\n        if param.requires_grad:\n            trainable+= param.numel()\n    return f'trainable model parameters: {trainable}\\nAll parameters: {all_params}\\npercentage of trainable parameters: {100*trainable/all_params:.2f}%'\n\n\n\nprint( print_number_of_trainable_parameters(original_model))","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:35:06.036934Z","iopub.execute_input":"2023-10-04T01:35:06.037451Z","iopub.status.idle":"2023-10-04T01:35:06.047521Z","shell.execute_reply.started":"2023-10-04T01:35:06.037401Z","shell.execute_reply":"2023-10-04T01:35:06.045997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.1-Test the Model with Zero shot inference\n\nin this section we just make a prmopt engineering with zero output","metadata":{}},{"cell_type":"code","source":"preamble = '''\nGiven the following film descriptions, your task is to classify each description into one of the following categories:\n- Thriller\n- Comedy\n- Documentary\n- Drama\n- Horror\n- Short\n- Western\n- Sport\n- Romance\n- War\n- Game Show\n- Biography\n- Adult\n- Talk Show\n- Family\n- Action\n- Music\n- Crime\n- Animation\n- Sci-Fi\n- Adventure\n- Reality TV\n- Fantasy\n- Mystery\n- History\n- News\n- Musical\n\nPlease arrange the classifications from the most likely to be correct to the least likely to be correct.'''\n\nend_prompt   = 'The answer:\\n'\ntemplate =  Template('$preamble\\n\\n$prompt\\n\\n$end_prompt')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:35:10.295627Z","iopub.execute_input":"2023-10-04T01:35:10.296145Z","iopub.status.idle":"2023-10-04T01:35:10.303412Z","shell.execute_reply.started":"2023-10-04T01:35:10.296103Z","shell.execute_reply":"2023-10-04T01:35:10.301826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def formate_input(df , index):\n    prompt     = df.loc[index , 'DESCRIPTION']\n    input_text =  template.substitute(preamble = preamble , prompt = prompt , end_prompt=end_prompt)\n    return input_text\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:35:10.305489Z","iopub.execute_input":"2023-10-04T01:35:10.307137Z","iopub.status.idle":"2023-10-04T01:35:10.328664Z","shell.execute_reply.started":"2023-10-04T01:35:10.307082Z","shell.execute_reply":"2023-10-04T01:35:10.327580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(formate_input(train_data , 0))","metadata":{"execution":{"iopub.status.busy":"2023-10-03T21:49:06.776736Z","iopub.status.idle":"2023-10-03T21:49:06.778068Z","shell.execute_reply.started":"2023-10-03T21:49:06.777844Z","shell.execute_reply":"2023-10-03T21:49:06.777866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's do zero shot\n\n\n\nzero_shot_discription = formate_input(train_data , 0)\nzero_shot_answer      = train_data.loc[0 , 'GENRE']\n\ninputs       =  tokenizer(zero_shot_discription , return_tensors='pt')\ngenerate     = original_model.generate(inputs['input_ids'] , max_new_tokens=1)[0]\nmodel_answer = tokenizer.decode(generate , skip_special_tokens =True)\nline_dash = '-'.join('' for _ in range(100))\nprint(line_dash)\nprint(f'Prompt:\\n{zero_shot_discription}')\nprint(line_dash)\nprint(f'Acual Answer:\\n{zero_shot_answer}')\nprint(line_dash)\nprint(f'Model Answer:\\n{model_answer}')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.2 - Using One Shot and Few Shot Inference\n\nlet's give our model some examples with answers","metadata":{}},{"cell_type":"code","source":"template2 = Template('$preamble\\n\\n$prompt\\n\\n$end_prompt$answer\\n\\n\\n')\n\ndef formate2(data , index):\n    prompt = data.loc[index , 'DESCRIPTION']\n    answer = data.loc[index , 'GENRE']\n    text   = template2.substitute(preamble = preamble , prompt = prompt , end_prompt=end_prompt , answer = answer)\n    return text\n\n\n\n\ndef create_example(example_index , example_class):\n\n    final_prompt = ''\n\n    for index in example_index:\n        text = formate2(train_data , index)\n        final_prompt += text\n\n\n    test_text     = formate_input(train_data , example_class)\n    final_prompt += test_text\n    test_answer   = train_data.loc[example_class , 'GENRE']\n    return final_prompt , test_answer\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:35:10.330453Z","iopub.execute_input":"2023-10-04T01:35:10.330834Z","iopub.status.idle":"2023-10-04T01:35:10.350214Z","shell.execute_reply.started":"2023-10-04T01:35:10.330803Z","shell.execute_reply":"2023-10-04T01:35:10.348364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_index = [0,1,2,3]\nexample_class = 200\n\nfew_shot_text , few_shot_answer = create_example(example_index , example_class)\nprint(few_shot_text)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's test our model again\n\nfew_shot_input        = tokenizer(few_shot_text , return_tensors='pt')\nfew_shot_generate     = original_model.generate(few_shot_input['input_ids'])[0]\nfew_shot_model_answer = tokenizer.decode(few_shot_generate , skip_special_tokens = True)\n\nprint(line_dash)\nprint(f'Acual Answer :\\n{few_shot_answer}')\nprint(line_dash)\nprint(f'Model Answer :\\n{few_shot_model_answer}')\nprint(line_dash)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 - Perform Parameter Efficient Fine-Tuning (PEFT)\n","metadata":{}},{"cell_type":"markdown","source":"# 2.1 - Preprocess the Classification Dataset\n\nconver all classification data into the explicit instruction for LLM. prepend the instruction with:\n\n\nGiven the following film descriptions, your task is to classify each description into one of the following categories:\n- Thriller\n- Comedy\n- Documentary\n- Drama\n- Horror\n- Short\n- Western\n- Sport\n- Romance\n- War\n- Game Show\n- Biography\n- Adult\n- Talk Show\n- Family\n- Action\n- Music\n- Crime\n- Animation\n- Sci-Fi\n- Adventure\n- Reality TV\n- Fantasy\n- Mystery\n- History\n- News\n- Musical\n\nPlease arrange the classifications from the most likely to be correct to the least likely to be correct.\n\n L.R. Brane loves his life - his car, his apartment, his job, but especially his girlfriend, Vespa. One day while showering, Vespa runs out of shampoo. L.R. runs across the street to a convenience store to buy some more, a quick trip of no more than a few minutes. When he returns, Vespa is gone and every trace of her existence has been wiped out. L.R.'s life becomes a tortured existence as one strange event after another occurs to confirm in his mind that a conspiracy is working against his finding Vespa.\n\nThe answer:\n\n drama\n\nthen tokenize the data into tokens","metadata":{}},{"cell_type":"code","source":"tokenize_prompt = [formate_input(train_data, index)for index in range(train_data.shape[0])]\n\n\ntokenize_train_data = train_data.copy()\ntokenize_train_data['new_prompt'] = tokenize_prompt\ntokenize_train_data = tokenize_train_data.drop(['ID' ,'TITLE' , 'DESCRIPTION'] , axis=1)\n\ntokens_for_prompt = [tokenizer(tokenize_train_data.loc[index , 'new_prompt'] ,  return_tensors='pt' , truncation=True ,padding='max_length').input_ids for index in range(tokenize_train_data.shape[0])]\ntokens_for_answer = [tokenizer(tokenize_train_data.loc[index , 'GENRE'] ,  return_tensors='pt' , truncation=True ,padding='max_length').input_ids for index in range(tokenize_train_data.shape[0])]\n\ntokenize_train_data['input_ids'] = tokens_for_prompt\ntokenize_train_data['labels']    = tokens_for_answer\n\ntokenize_train_data = tokenize_train_data.drop(['GENRE' , 'new_prompt'] , axis=1)\n\n\n\ntokenize_train_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:35:10.353328Z","iopub.execute_input":"2023-10-04T01:35:10.353756Z","iopub.status.idle":"2023-10-04T01:37:21.822283Z","shell.execute_reply.started":"2023-10-04T01:35:10.353721Z","shell.execute_reply":"2023-10-04T01:37:21.821145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import DatasetDict , Dataset\n\n# preparing dataset for LLM\n\nnew_tokens_for_prompt = []\nnew_tokens_for_answer = []\nfor i in range(len(tokens_for_prompt)):\n    new_tokens_for_prompt.append(tokens_for_prompt[i][0])\n    new_tokens_for_answer.append(tokens_for_answer[i][0])\n\n\ntrain_tokens = {\n    'input_ids': new_tokens_for_prompt ,\n    'labels': new_tokens_for_answer\n}\n\nTrain_Dict = Dataset.from_dict(train_tokens)\nTrain_Dict\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:37:21.823615Z","iopub.execute_input":"2023-10-04T01:37:21.823979Z","iopub.status.idle":"2023-10-04T01:37:28.176318Z","shell.execute_reply.started":"2023-10-04T01:37:21.823951Z","shell.execute_reply":"2023-10-04T01:37:28.174882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.2 - Setup the PEFT/LoRA model for Fine-Tuning\n\nYou need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (r) hyper-parameter, which defines the rank/dimension of the adapter to be trained.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig , get_peft_model , TaskType\n\nlora_configs = LoraConfig(\n    r=32, # Rank\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n                               )\n\npeft_model = get_peft_model(original_model , lora_configs)\n\nprint(print_number_of_trainable_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:37:28.177788Z","iopub.execute_input":"2023-10-04T01:37:28.178119Z","iopub.status.idle":"2023-10-04T01:37:28.958478Z","shell.execute_reply.started":"2023-10-04T01:37:28.178091Z","shell.execute_reply":"2023-10-04T01:37:28.956964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_output_dir = f'/kaggle/working/peft-{str(int(time.time()))}'\n\npeft_training_args = TrainingArguments(\n    output_dir=peft_output_dir,\n    auto_find_batch_size=True,\n    learning_rate=1e-3, # Higher learning rate.\n    num_train_epochs=1,\n    logging_steps=1,\n    max_steps=1)\n\n\npeft_trainer = Trainer(\n    model = peft_model ,\n    args = peft_training_args ,\n    train_dataset = Train_Dict\n\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:37:28.961115Z","iopub.execute_input":"2023-10-04T01:37:28.961494Z","iopub.status.idle":"2023-10-04T01:37:28.982117Z","shell.execute_reply.started":"2023-10-04T01:37:28.961464Z","shell.execute_reply":"2023-10-04T01:37:28.981243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_trainer.train()\n\npeft_model_path = f'/kaggle/working/peft_model_trainer'\npeft_trainer.model_save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:48:54.779263Z","iopub.execute_input":"2023-10-04T01:48:54.779768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel , PeftConfig \n\npeft_model_base = AutModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\ntokenizer       = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n\n\npeft_model = PeftModel.from_pretrained(peft_model_base ,  \n                                      peft_model_path ,\n                                      torch_dtype = torch.bfloat16 ,\n                                      is_trainable=False)\n\nprint(print_number_of_trainable_parameters(peft_model))\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's test our fine-tuned model \n\n\n\nimport random\nhuman_answer = []\npeft_model_answer=[]\n\n# choose 10 numbers randomly \nrandom_numbers = [ random.randint(0 , len(train_data['GENRE'])) for _ in range(2)]\n\n\nfor i in random_numbers:\n    structer = formate_input(train_data , i)\n    inputs   = tokenizer(structer , return_tensors='pt')\n    generate = peft_model.generate(inputs['input_ids'])[0]\n    outputs  = tokenizer.decode(generate , skip_special_tokens=True)\n    peft_model_answer.append(outputs)\n    human_answer.append(train_data['GENRE'][i])\n","metadata":{},"execution_count":null,"outputs":[]}]}